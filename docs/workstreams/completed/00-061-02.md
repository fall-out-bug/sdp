---
ws_id: "00-061-02"
feature_id: "F061"
title: "AI Failure Taxonomy"
status: "backlog"
priority: "P1"
depends_on: ["00-061-01"]
blocks: ["00-061-03"]
project_id: "00"
---

# 00-061-02: AI Failure Taxonomy

## Goal

Classify AI failures by model, language, domain, and failure type. Build the taxonomy from evidence data.

## Context

When AI generation fails verification, we know it failed — but not why. This WS adds failure classification: was it wrong logic? Missing edge case? Hallucinated API? Type error? This powers the quarterly benchmark.

## Acceptance Criteria

- [ ] AC1: Failure classification schema: model, language, domain, failure_type, severity
- [ ] AC2: Failure types: wrong_logic, missing_edge_case, hallucinated_api, type_error, test_passing_but_wrong, compilation_error, import_error
- [ ] AC3: `sdp metrics classify` auto-classifies failures from verification output
- [ ] AC4: Classification heuristics: parse test output for error patterns
- [ ] AC5: Manual override: `sdp metrics classify --id=evt-123 --type=wrong_logic`
- [ ] AC6: Taxonomy stored in `.sdp/metrics/taxonomy.json`

## Scope Files

**Implementation:**
- sdp-plugin/internal/metrics/taxonomy.go (new)
- sdp-plugin/internal/metrics/taxonomy_test.go (new)
- sdp-plugin/cmd/sdp/metrics.go (update — add classify subcommand)
- schema/failure-taxonomy.schema.json (new)

**Tests:**
- sdp-plugin/internal/metrics/taxonomy_test.go

## Notes

- ~2h work
- Auto-classification is heuristic — won't be perfect, but patterns are recognizable
- "test_passing_but_wrong" = acceptance test catches what unit tests miss (F054 case)
- Taxonomy evolves: start with 7 types, add more as we see patterns
