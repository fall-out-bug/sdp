---
ws_id: "00-061-03"
feature_id: "F061"
title: "Benchmark Report Generator"
status: complete
priority: "P1"
depends_on: ["00-061-01", "00-061-02"]
blocks: []
project_id: "00"
---

# 00-061-03: Benchmark Report Generator

## Goal

Generate the "AI Code Quality Benchmark" report from collected metrics and taxonomy data. Quarterly publication.

## Acceptance Criteria

- [ ] AC1: `sdp metrics report` generates markdown benchmark report
- [ ] AC2: Report includes: aggregate catch rate, model comparison, failure taxonomy breakdown
- [ ] AC3: Report includes: trend over time (if historical data exists)
- [ ] AC4: Privacy: only aggregates, no raw evidence, no company names
- [ ] AC5: `--format=md` (default), `--format=html`, `--format=json`
- [ ] AC6: Report placed in `.sdp/metrics/benchmark-YYYY-QN.md`
- [ ] AC7: First report: SDP dogfooding data only

## Scope Files

**Implementation:**
- sdp-plugin/internal/metrics/report.go (new)
- sdp-plugin/internal/metrics/report_test.go (new)
- sdp-plugin/cmd/sdp/metrics.go (update — add report subcommand)

**Tests:**
- sdp-plugin/internal/metrics/report_test.go

## Notes

- ~1.5h work
- First benchmark: SDP builds SDP — dogfood data from F054 onwards
- Trend: compare Q1 vs Q2 as data accumulates
- Publication: blog post / GitHub release — manual, not automated
- This is the "trust standard" data layer — numbers, not marketing
