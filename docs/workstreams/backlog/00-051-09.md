---
ws_id: 00-051-09
feature_id: F051
title: "Memory Management: Compaction & Archival"
status: backlog
priority: 1
depends_on: [00-051-01, 00-051-04]
blocks: []
project_id: sdp
beads_id: sdp-6fx.8
---

## Goal

Implement memory compaction and archival to prevent database bloat and manage storage costs.

## Context

Part of F051 Long-term Memory System. Without compaction, memory.db grows unbounded. This workstream implements tiered storage, event summarization, and embedding optimization.

**Problem:**
- 1000 artifacts × 6KB embeddings = 6MB (before optimization)
- 10,000 events × 500 bytes = 5MB (before compaction)
- Growth: ~10MB/month for active project

## Acceptance Criteria

- [ ] AC1: Compaction policy configuration in .sdp/config.yml
- [ ] AC2: Event summarization (N events → 1 summary via LLM)
- [ ] AC3: Tiered storage: hot (30 days) / warm (90 days) / cold (archive)
- [ ] AC4: Embedding quantization (float32 → int8, 4x reduction)
- [ ] AC5: Automatic compaction trigger at configurable size threshold
- [ ] AC6: Archive format: .tar.zst with integrity checksums

## Scope Files

```yaml
scope_files:
  - sdp-plugin/internal/memory/compactor.go        # NEW
  - sdp-plugin/internal/memory/compactor_test.go   # NEW
  - sdp-plugin/internal/memory/tiers.go            # NEW
  - sdp-plugin/internal/memory/tiers_test.go       # NEW
  - sdp-plugin/internal/memory/quantize.go         # NEW
  - sdp-plugin/internal/memory/quantize_test.go    # NEW
  - sdp-plugin/cmd/sdp/memory.go                   # MODIFY (add compact/archive commands)
```

## Implementation Notes

### Compaction Policy

```yaml
# .sdp/config.yml
memory:
  compaction:
    # Events older than N days are summarized
    event_retention_days: 30

    # Summarize N events into 1 summary
    compaction_ratio: 10

    # Move to cold storage after N days
    archive_after_days: 90

    # Maximum DB size before forced compaction (MB)
    max_db_size_mb: 100

    # Run compaction automatically
    auto_compact: true

  embeddings:
    # Quantize to int8 (4x smaller)
    quantize: true

    # Only compute on demand
    lazy: true
```

### Tiered Storage

```go
type MemoryTier string

const (
    TierHot  MemoryTier = "hot"  // Last 30 days, always loaded
    TierWarm MemoryTier = "warm" // 30-90 days, lazy-loaded
    TierCold MemoryTier = "cold" // 90+ days, archived
)

type TierManager struct {
    db         *sql.DB
    archiveDir string
    policy     CompactionPolicy
}

func (t *TierManager) Compact() error {
    // 1. Find events older than retention period
    // 2. Group by workstream/feature
    // 3. Generate summary via LLM
    // 4. Replace N events with 1 summary
    // 5. Update indexes
    // 6. Vacuum SQLite
}

func (t *TierManager) Archive() error {
    // 1. Find artifacts older than archive threshold
    // 2. Export to .tar.zst
    // 3. Verify integrity
    // 4. Remove from hot storage
    // 5. Keep metadata pointer
}
```

### Event Summarization

```go
type EventSummarizer struct {
    llmClient LLMClient
}

func (s *EventSummarizer) Summarize(events []AgentEvent) (*EventSummary, error) {
    // Build prompt with events
    prompt := fmt.Sprintf(`
Summarize these %d agent events into a single summary:

%s

Output format:
- What was accomplished
- Key decisions made
- Important metrics
- Any blockers encountered
`, len(events), formatEvents(events))

    // Call LLM
    summary, err := s.llmClient.Summarize(prompt)
    if err != nil {
        return nil, err
    }

    return &EventSummary{
        WSID:       events[0].TaskID,
        Summary:    summary,
        EventCount: len(events),
        StartTime:  events[0].Timestamp,
        EndTime:    events[len(events)-1].Timestamp,
    }, nil
}
```

### Embedding Quantization

```go
// Quantize float32 embedding to int8 (4x smaller)
func QuantizeEmbedding(embedding []float32) ([]int8, error) {
    if len(embedding) == 0 {
        return nil, errors.New("empty embedding")
    }

    // Find min/max for normalization
    min, max := float32(0), float32(0)
    for _, v := range embedding {
        if v < min {
            min = v
        }
        if v > max {
            max = v
        }
    }

    // Quantize to int8 range [-128, 127]
    scale := float32(127) / maxAbs(min, max)
    quantized := make([]int8, len(embedding))
    for i, v := range embedding {
        quantized[i] = int8(v * scale)
    }

    return quantized, nil
}

// Dequantize back to float32 for similarity search
func DequantizeEmbedding(quantized []int8, scale float32) []float32 {
    result := make([]float32, len(quantized))
    for i, v := range quantized {
        result[i] = float32(v) / scale
    }
    return result
}
```

### Storage Savings

| Component | Before | After | Savings |
|-----------|--------|-------|---------|
| Embeddings (1000 docs) | 6 MB | 1.5 MB | 75% |
| Events (10k, compacted) | 5 MB | 0.5 MB | 90% |
| Full artifacts (archived) | 50 MB | 5 MB | 90% |
| **Total** | **61 MB** | **7 MB** | **88%** |

### CLI Commands

```bash
# Manual compaction
sdp memory compact

# Archive old data
sdp memory archive --older-than=90d

# Show memory stats
sdp memory stats
# Output:
# Hot:  2.1 MB (150 artifacts, 500 events)
# Warm: 5.3 MB (300 artifacts, 2000 summaries)
# Cold: 1.2 MB archived (500 artifacts)

# Restore from archive
sdp memory restore --feature=F050
```

## Definition of Done

- [ ] All AC met
- [ ] Tests pass with ≥80% coverage
- [ ] CLI commands work: `sdp memory compact/archive/stats`
- [ ] Storage reduced by >80% after compaction
- [ ] No data loss (archive integrity verified)
- [ ] Documentation updated with retention policy
