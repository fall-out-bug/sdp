---
ws_id: "00-061-01"
feature_id: "F061"
title: "Metrics Collection Pipeline"
status: "backlog"
priority: "P1"
depends_on: ["00-054-05"]
blocks: ["00-061-03"]
project_id: "00"
---

# 00-061-01: Metrics Collection Pipeline

## Goal

Extract structured metrics from evidence events: catch rate, iteration count, model performance, acceptance test outcomes.

## Context

Evidence log (F054) captures raw events. This WS transforms them into actionable metrics: "How often do tests fail on first generation?" "Which model has the highest pass rate?" "Does the acceptance test catch more than unit tests?"

## Acceptance Criteria

- [ ] AC1: `sdp metrics collect` scans evidence log and computes metrics
- [ ] AC2: Metrics: catch_rate (verification failures / total verifications)
- [ ] AC3: Metrics: iteration_count (red→green cycles per workstream)
- [ ] AC4: Metrics: model_pass_rate (pass rate per model ID)
- [ ] AC5: Metrics: acceptance_catch_rate (acceptance failures / total builds)
- [ ] AC6: Output: JSON metrics file (`.sdp/metrics/latest.json`)
- [ ] AC7: Incremental: only process events since last collection

## Scope Files

**Implementation:**
- sdp-plugin/cmd/sdp/metrics.go (new)
- sdp-plugin/internal/metrics/collector.go (new)
- sdp-plugin/internal/metrics/collector_test.go (new)
- sdp-plugin/cmd/sdp/main.go (update — register command)

**Tests:**
- sdp-plugin/internal/metrics/collector_test.go

## Notes

- ~2h work
- Reads from evidence log (internal/evidence/reader.go)
- Produces metrics JSON — consumed by F061 benchmark and dashboards
- Incremental: store watermark (last processed event ID)
